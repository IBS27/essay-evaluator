{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1');\n",
    "df.dropna(axis=1,inplace=True)\n",
    "df.drop(columns=['domain1_score','rater1_domain1','rater2_domain1'],inplace=True,axis=1)\n",
    "df.head()\n",
    "temp = pd.read_csv(\"Processed_data.csv\")\n",
    "temp.drop(\"Unnamed: 0\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              6  \n",
       "1              7  \n",
       "2              5  \n",
       "3              8  \n",
       "4              6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['domain1_score']=temp['final_score']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>final_score</th>\n",
       "      <th>clean_essay</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>6</td>\n",
       "      <td>Dear local newspaper  I think effects computer...</td>\n",
       "      <td>1441</td>\n",
       "      <td>344</td>\n",
       "      <td>16</td>\n",
       "      <td>4.188953</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear I believe that using computers will benef...</td>\n",
       "      <td>7</td>\n",
       "      <td>Dear I believe using computers benefit us many...</td>\n",
       "      <td>1765</td>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>4.273608</td>\n",
       "      <td>21</td>\n",
       "      <td>98</td>\n",
       "      <td>84</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, More and more people use computers, but ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Dear  More people use computers  everyone agre...</td>\n",
       "      <td>1185</td>\n",
       "      <td>276</td>\n",
       "      <td>14</td>\n",
       "      <td>4.293478</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I have found that many e...</td>\n",
       "      <td>8</td>\n",
       "      <td>Dear Local Newspaper  I found many experts say...</td>\n",
       "      <td>2284</td>\n",
       "      <td>490</td>\n",
       "      <td>26</td>\n",
       "      <td>4.661224</td>\n",
       "      <td>31</td>\n",
       "      <td>142</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear I know having computers has a positive ef...</td>\n",
       "      <td>6</td>\n",
       "      <td>Dear I know computers positive effect people  ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>469</td>\n",
       "      <td>30</td>\n",
       "      <td>4.313433</td>\n",
       "      <td>18</td>\n",
       "      <td>110</td>\n",
       "      <td>90</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear I believe that using computers will benef...   \n",
       "2         3          1  Dear, More and more people use computers, but ...   \n",
       "3         4          1  Dear Local Newspaper, I have found that many e...   \n",
       "4         5          1  Dear I know having computers has a positive ef...   \n",
       "\n",
       "   final_score                                        clean_essay  char_count  \\\n",
       "0            6  Dear local newspaper  I think effects computer...        1441   \n",
       "1            7  Dear I believe using computers benefit us many...        1765   \n",
       "2            5  Dear  More people use computers  everyone agre...        1185   \n",
       "3            8  Dear Local Newspaper  I found many experts say...        2284   \n",
       "4            6  Dear I know computers positive effect people  ...        2023   \n",
       "\n",
       "   word_count  sent_count  avg_word_len  spell_err_count  noun_count  \\\n",
       "0         344          16      4.188953               11          76   \n",
       "1         413          17      4.273608               21          98   \n",
       "2         276          14      4.293478                5          76   \n",
       "3         490          26      4.661224               31         142   \n",
       "4         469          30      4.313433               18         110   \n",
       "\n",
       "   adj_count  verb_count  adv_count  \n",
       "0         75          18         24  \n",
       "1         84          20         19  \n",
       "2         51          20         16  \n",
       "3         96          39         29  \n",
       "4         90          32         36  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Dataset\n",
    "y = df['domain1_score']\n",
    "df.drop('domain1_score',inplace=True,axis=1)\n",
    "X=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9083, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = X_train['essay'].tolist()\n",
    "test_e = X_test['essay'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents=[]\n",
    "test_sents=[]\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def sent2word(x):\n",
    "    x=re.sub(\"[^A-Za-z]\",\" \",x)\n",
    "    x.lower()\n",
    "    filtered_sentence = [] \n",
    "    words=x.split()\n",
    "    for w in words:\n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def essay2word(essay):\n",
    "    essay = essay.strip()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw = tokenizer.tokenize(essay)\n",
    "    final_words=[]\n",
    "    for i in raw:\n",
    "        if(len(i)>0):\n",
    "            final_words.append(sent2word(i))\n",
    "    return final_words\n",
    "\n",
    "for i in train_e:\n",
    "    train_sents+=essay2word(i)\n",
    "\n",
    "for i in test_e:\n",
    "    test_sents+=essay2word(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'first',\n",
       " 'day',\n",
       " 'high',\n",
       " 'school',\n",
       " 'gut',\n",
       " 'full',\n",
       " 'butterflies',\n",
       " 'make',\n",
       " 'want',\n",
       " 'run',\n",
       " 'bathrooms',\n",
       " 'hide',\n",
       " 'world']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing WORD2VEC and LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/w603fwyn6l3fpy_8v1q3j2b40000gn/T/ipykernel_4583/1462232201.py:15: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "#Training Word2Vec model\n",
    "num_features = 300 \n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model = Word2Vec(train_sents, \n",
    "                 workers=num_workers, \n",
    "                 vector_size=num_features, \n",
    "                 min_count = min_word_count, \n",
    "                 window = context, \n",
    "                 sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/w603fwyn6l3fpy_8v1q3j2b40000gn/T/ipykernel_4583/2615469268.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  vec = np.divide(vec,noOfWords)\n"
     ]
    }
   ],
   "source": [
    "def makeVec(words, model, num_features):\n",
    "    vec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    noOfWords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for i in words:\n",
    "        if i in index2word_set:\n",
    "            noOfWords += 1\n",
    "            vec = np.add(vec,model.wv.get_vector(i))        \n",
    "    vec = np.divide(vec,noOfWords)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def getVecs(essays, model, num_features):\n",
    "    c=0\n",
    "    essay_vecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for i in essays:\n",
    "        essay_vecs[c] = makeVec(i, model, num_features)\n",
    "        c+=1\n",
    "    return essay_vecs\n",
    "\n",
    "\n",
    "clean_train=[]\n",
    "for i in train_e:\n",
    "    clean_train.append(sent2word(i))\n",
    "training_vectors = getVecs(clean_train, model, num_features)\n",
    "\n",
    "clean_test=[] \n",
    "\n",
    "for i in test_e:\n",
    "    clean_test.append(sent2word(i))\n",
    "testing_vectors = getVecs(clean_test, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9083, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_vectors = np.array(training_vectors)\n",
    "testing_vectors = np.array(testing_vectors)\n",
    "\n",
    "# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "training_vectors = np.reshape(training_vectors, (training_vectors.shape[0], 1, training_vectors.shape[1]))\n",
    "testing_vectors = np.reshape(testing_vectors, (testing_vectors.shape[0], 1, testing_vectors.shape[1]))\n",
    "lstm_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9083, 1, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vectors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING AND PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 00:12:47.047519: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 2s 6ms/step - loss: 11.2409 - mae: 2.6364\n",
      "Epoch 2/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.5528 - mae: 1.8696\n",
      "Epoch 3/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.2524 - mae: 1.8081\n",
      "Epoch 4/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.2462 - mae: 1.8111\n",
      "Epoch 5/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.1654 - mae: 1.7953\n",
      "Epoch 6/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.0982 - mae: 1.7892\n",
      "Epoch 7/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 5.0732 - mae: 1.7778\n",
      "Epoch 8/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 5.0220 - mae: 1.7744\n",
      "Epoch 9/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 5.0520 - mae: 1.7773\n",
      "Epoch 10/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.9695 - mae: 1.7596\n",
      "Epoch 11/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.9957 - mae: 1.7641\n",
      "Epoch 12/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.9844 - mae: 1.7597\n",
      "Epoch 13/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.8915 - mae: 1.7449\n",
      "Epoch 14/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.9288 - mae: 1.7525\n",
      "Epoch 15/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.8730 - mae: 1.7380\n",
      "Epoch 16/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.8389 - mae: 1.7317\n",
      "Epoch 17/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.8524 - mae: 1.7293\n",
      "Epoch 18/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.8285 - mae: 1.7285\n",
      "Epoch 19/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.8395 - mae: 1.7330\n",
      "Epoch 20/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.7664 - mae: 1.7153\n",
      "Epoch 21/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.8107 - mae: 1.7250\n",
      "Epoch 22/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.7642 - mae: 1.7152\n",
      "Epoch 23/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.7795 - mae: 1.7156\n",
      "Epoch 24/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.7372 - mae: 1.7141\n",
      "Epoch 25/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.7478 - mae: 1.7075\n",
      "Epoch 26/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.6631 - mae: 1.6966\n",
      "Epoch 27/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.6422 - mae: 1.6918\n",
      "Epoch 28/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.5449 - mae: 1.6710\n",
      "Epoch 29/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.5875 - mae: 1.6756\n",
      "Epoch 30/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.6448 - mae: 1.6846\n",
      "Epoch 31/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.5035 - mae: 1.6583\n",
      "Epoch 32/150\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 4.5603 - mae: 1.6728\n",
      "Epoch 33/150\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 4.4211 - mae: 1.6496\n",
      "Epoch 34/150\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 4.4413 - mae: 1.6472\n",
      "Epoch 35/150\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 4.4762 - mae: 1.6601\n",
      "Epoch 36/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 4.4221 - mae: 1.6415\n",
      "Epoch 37/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.3963 - mae: 1.6421\n",
      "Epoch 38/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.3732 - mae: 1.6369\n",
      "Epoch 39/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.3249 - mae: 1.6213\n",
      "Epoch 40/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.3163 - mae: 1.6263\n",
      "Epoch 41/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.2796 - mae: 1.6133\n",
      "Epoch 42/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.3086 - mae: 1.6184\n",
      "Epoch 43/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.2696 - mae: 1.6151\n",
      "Epoch 44/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.2673 - mae: 1.6123\n",
      "Epoch 45/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.1999 - mae: 1.5989\n",
      "Epoch 46/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.2462 - mae: 1.6038\n",
      "Epoch 47/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.1964 - mae: 1.5947\n",
      "Epoch 48/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.1410 - mae: 1.5810\n",
      "Epoch 49/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.1869 - mae: 1.6017\n",
      "Epoch 50/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.0802 - mae: 1.5744\n",
      "Epoch 51/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.1091 - mae: 1.5819\n",
      "Epoch 52/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.0727 - mae: 1.5780\n",
      "Epoch 53/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.0672 - mae: 1.5746\n",
      "Epoch 54/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.1273 - mae: 1.5916\n",
      "Epoch 55/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.0242 - mae: 1.5675\n",
      "Epoch 56/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.1018 - mae: 1.5754\n",
      "Epoch 57/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 4.0189 - mae: 1.5634\n",
      "Epoch 58/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 4.0320 - mae: 1.5640\n",
      "Epoch 59/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.9602 - mae: 1.5552\n",
      "Epoch 60/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.9581 - mae: 1.5504\n",
      "Epoch 61/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.8854 - mae: 1.5363\n",
      "Epoch 62/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.9342 - mae: 1.5473\n",
      "Epoch 63/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.9037 - mae: 1.5399\n",
      "Epoch 64/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.8858 - mae: 1.5377\n",
      "Epoch 65/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.8315 - mae: 1.5293\n",
      "Epoch 66/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.8338 - mae: 1.5314\n",
      "Epoch 67/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.8878 - mae: 1.5448\n",
      "Epoch 68/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.8700 - mae: 1.5333\n",
      "Epoch 69/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7831 - mae: 1.5193\n",
      "Epoch 70/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7981 - mae: 1.5219\n",
      "Epoch 71/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7552 - mae: 1.5182\n",
      "Epoch 72/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7893 - mae: 1.5284\n",
      "Epoch 73/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7685 - mae: 1.5155\n",
      "Epoch 74/150\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 3.7693 - mae: 1.5166\n",
      "Epoch 75/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7486 - mae: 1.5127\n",
      "Epoch 76/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7227 - mae: 1.5052\n",
      "Epoch 77/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6998 - mae: 1.5020\n",
      "Epoch 78/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.7049 - mae: 1.5020\n",
      "Epoch 79/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6604 - mae: 1.4952\n",
      "Epoch 80/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.6873 - mae: 1.4967\n",
      "Epoch 81/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.6930 - mae: 1.5049\n",
      "Epoch 82/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.7020 - mae: 1.5016\n",
      "Epoch 83/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6603 - mae: 1.4952\n",
      "Epoch 84/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6851 - mae: 1.4973\n",
      "Epoch 85/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.6292 - mae: 1.4843\n",
      "Epoch 86/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6173 - mae: 1.4828\n",
      "Epoch 87/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.6049 - mae: 1.4807\n",
      "Epoch 88/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5821 - mae: 1.4697\n",
      "Epoch 89/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5951 - mae: 1.4801\n",
      "Epoch 90/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5600 - mae: 1.4602\n",
      "Epoch 91/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5484 - mae: 1.4623\n",
      "Epoch 92/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5702 - mae: 1.4644\n",
      "Epoch 93/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5699 - mae: 1.4799\n",
      "Epoch 94/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5794 - mae: 1.4782\n",
      "Epoch 95/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5413 - mae: 1.4670\n",
      "Epoch 96/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5206 - mae: 1.4639\n",
      "Epoch 97/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5524 - mae: 1.4651\n",
      "Epoch 98/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4721 - mae: 1.4533\n",
      "Epoch 99/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.5333 - mae: 1.4594\n",
      "Epoch 100/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4963 - mae: 1.4507\n",
      "Epoch 101/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4499 - mae: 1.4424\n",
      "Epoch 102/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.4767 - mae: 1.4499\n",
      "Epoch 103/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.5112 - mae: 1.4573\n",
      "Epoch 104/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4616 - mae: 1.4515\n",
      "Epoch 105/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.5252 - mae: 1.4556\n",
      "Epoch 106/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4551 - mae: 1.4487\n",
      "Epoch 107/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4421 - mae: 1.4438\n",
      "Epoch 108/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4712 - mae: 1.4452\n",
      "Epoch 109/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4063 - mae: 1.4316\n",
      "Epoch 110/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4389 - mae: 1.4461\n",
      "Epoch 111/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4174 - mae: 1.4382\n",
      "Epoch 112/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3626 - mae: 1.4285\n",
      "Epoch 113/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3760 - mae: 1.4318\n",
      "Epoch 114/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3982 - mae: 1.4265\n",
      "Epoch 115/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3886 - mae: 1.4300\n",
      "Epoch 116/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.4289 - mae: 1.4378\n",
      "Epoch 117/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3781 - mae: 1.4252\n",
      "Epoch 118/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3608 - mae: 1.4292\n",
      "Epoch 119/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3816 - mae: 1.4291\n",
      "Epoch 120/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2875 - mae: 1.4098\n",
      "Epoch 121/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3166 - mae: 1.4150\n",
      "Epoch 122/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3308 - mae: 1.4154\n",
      "Epoch 123/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3159 - mae: 1.4125\n",
      "Epoch 124/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3255 - mae: 1.4152\n",
      "Epoch 125/150\n",
      "142/142 [==============================] - 1s 10ms/step - loss: 3.3529 - mae: 1.4157\n",
      "Epoch 126/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3056 - mae: 1.4115\n",
      "Epoch 127/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3245 - mae: 1.4202\n",
      "Epoch 128/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3179 - mae: 1.4210\n",
      "Epoch 129/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3398 - mae: 1.4158\n",
      "Epoch 130/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.3292 - mae: 1.4193\n",
      "Epoch 131/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2836 - mae: 1.4090\n",
      "Epoch 132/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2731 - mae: 1.4098\n",
      "Epoch 133/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2899 - mae: 1.4148\n",
      "Epoch 134/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2572 - mae: 1.3968\n",
      "Epoch 135/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2533 - mae: 1.3989\n",
      "Epoch 136/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2871 - mae: 1.4065\n",
      "Epoch 137/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2989 - mae: 1.4126\n",
      "Epoch 138/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.2552 - mae: 1.4034\n",
      "Epoch 139/150\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 3.3110 - mae: 1.4093\n",
      "Epoch 140/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2430 - mae: 1.3962\n",
      "Epoch 141/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2353 - mae: 1.3976\n",
      "Epoch 142/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2627 - mae: 1.4056\n",
      "Epoch 143/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2492 - mae: 1.4048\n",
      "Epoch 144/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2081 - mae: 1.3972\n",
      "Epoch 145/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2153 - mae: 1.3933\n",
      "Epoch 146/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2106 - mae: 1.3862\n",
      "Epoch 147/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.2448 - mae: 1.3973\n",
      "Epoch 148/150\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 3.1962 - mae: 1.3904\n",
      "Epoch 149/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.1870 - mae: 1.3867\n",
      "Epoch 150/150\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 3.1792 - mae: 1.3874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d218e20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_vectors, y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.],\n",
       "       [5.],\n",
       "       [6.],\n",
       "       ...,\n",
       "       [7.],\n",
       "       [8.],\n",
       "       [9.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.save('final_lstm.h5')\n",
    "y_pred = lstm_model.predict(testing_vectors)\n",
    "y_pred = np.around(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essay_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
